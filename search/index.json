[{"content":" ⚠ 주의 : 본 포스트에서는 k8s에 Windows를 설치하는 방법에 대한 정보를 담고 있습니다.\nMS 법무팀과 평생 기억에 남을 찐한 티 타임을 가지고 싶은게 아니라면 Windows Container나 Mac OS Container 페이지에도 나와있듯, 별도 라이센스 없이 상업적으로 사용하는 것은 삼가도록 합시다.\n문제의 시작 나는 깔끔한 걸 굉장히 좋아하는 성격이다.\n회사에서나 집에서나 컴퓨터 바탕화면에는 항상 필요한 최소한의 아이콘만 배치하고 있다.\n특히나 Docker를 접하게 된 이후로는 개발자 컴퓨터라면 응당 설치되어 있을법한 python이나 nodejs, java 심지어 Database Client 같은 것들도 모두 컨테이너를 통해 사용하고 있다.\n하지만 반드시 Windows 혹은 Mac에서만 동작하도록 만들어진 프로그램들도 있기 마련이다.\n예컨대 카카오톡의 경우 Docker Container로 사용하기가 대단히 어렵거나 하더라도 매끄럽게 동작하질 않는다.\n이러한 것들 중 내가 특히나 불편하게 여기는 게 있는데 바로 은행 보안 프로그램이다.\n로그인 하려면 이것들부터 설치해야 한다 한 달에 한 번씩 정기적으로 가계부를 정리하는 습관을 가지고 있는데,\n금융 관련 보안에 대해서는 굉장히 민감한 편이라 반드시 집에 있는 메인 PC로만 작업하곤 한다.\n문제는 다음과 같다.\n매번 은행업무 볼 때마다 각종 은행 보안 프로그램이 설치 / 업데이트 / 실행 된다. 은행들끼리 통일도 안 되어 있는 건지 다들 사용하는 프로그램이 조금씩 다 다르다. 개인적으로 사용하는 보안 프로그램과 충돌나는 경우가 잦다. 은근히 컴퓨터 자원을 많이 잡아 먹는다. 더 이상 메인 PC에 이런 자질구레한 보안 프로그램들이 설치되는 걸 용납할 수가 없었다.\n그렇다고 은행업무 전용 PC를 산다는 건 지나친 투자이다. 빈대 잡자고 초가삼간 태울 순 없는 노릇 아닌가.\n그렇담 가상화는?\n이 생각도 안 해본 건 아니다. 아니 생각 정도가 아니라 실제로 VMWare로 시도도 해봤다. 동작도 잘 한다.\n그런데, 메인 PC에 은행업무 하나 보자고 가상환경 설치하는 것 자체가 너무나도 번거럽고 부담스러운 일이었다.\n해결방안 - k8s에 Windows를 설치하자 이번에 설치할 컨테이너 이미지는 dockurr/windows이다.\n이게 그 Windows 컨테이너라는 건가?\n싶을 수도 있는데, 아니다.\nWindows Container는 Host OS가 Windows일 때 Windows App을 격리하여 실행하기 위해 있는 것으로, 우리가 흔히 Container라고 부르는 것은 십중팔구 Linux Container를 의미한다. Linux Container와 Windows Container는 근본적으로 다르다.\nLinux Container는 Linux OS 위에서만 동작한다. Unix Container라는 것도 있다. Unix OS에서만 동작한다. 마찬가지로 Windows Container는 Windows 위에서만 동작한다. 같은 개발자분이 작성한 다른 이미지들을 보면 Windows 이외에도 MacOS, Casa등 다양하게 준비되어 있는 걸 볼 수 있는데, 이 이미지들은 모두 Linux 시스템 위에서 동작하는 걸 전제로 만들어져 있다.\n이런게 가능한 이유는 Container 수준이 아니라 아예 커널 레벨에서의 가상화를 쓰기 때문이다.\n핵심 컴포넌트는 KVM(kernel-based Virtual Machine)이다.\nKVM (kernel-based Virtual Machine) KVM은 Linux Kernel을 하이퍼바이저로 변환하는 가상화 기술로, 단일 물리적 컴퓨터에서 여러 개의 격리된 가상 머신(VM), 즉 \u0026ldquo;게스트\u0026rdquo; 운영체제를 실행할 수 있게 해준다.\n각 VM을 메모리, 스토리지, 네트워크 카드와 같은 가상화된 하드웨어 구성 요소를 갖춘 일반 Linux 프로세스로 처리함으로써 효율적이고 고성능의 가상화를 제공한다.\n동작 원리 Linux Kernel 통합: KVM은 Linux Kernel 자체 내의 모듈이기 때문에 하이퍼바이저 역할을 하기 위한 별도의 운영체제가 필요하지 않다.\n하드웨어 가상화: KVM은 Intel VT-x 또는 AMD-V와 같은 하드웨어 가상화 확장 기능을 활용하여 완전한 하드웨어 수준의 가상화를 제공한다.\nQEMU: KVM이 핵심 가상화 기능을 제공하는 반면, QEMU 에뮬레이터와 함께 작동하여 게스트 VM에 하드웨어 에뮬레이션 및 가상 장치를 제공한다.\n가상 머신: 각 게스트 OS는 자체 전용 가상 하드웨어를 갖춘 별도의 Linux 프로세스로 실행되어 다른 VM과의 격리를 보장한다.\n주요 특징 오픈소스 및 무료: KVM은 오픈소스이며 무료로 사용할 수 있다.\n성능: Linux Kernel과의 깊은 통합으로 인해 높은 성능과 효율성을 자랑한다.\n유연성: Linux와 Windows를 포함한 다양한 운영체제를 게스트 VM으로 실행할 수 있다.\n비용 효율성: Linux의 구성 요소로서 값비싼 라이선스 비용이 필요하지 않다.\n일반적인 사용 사례 클라우드 컴퓨팅: KVM은 많은 퍼블릭 및 프라이빗 클라우드 인프라의 핵심 기술로, 확장 가능한 온디맨드 컴퓨팅 리소스를 제공한다.\n소프트웨어 개발 및 테스트: 개발자들은 KVM을 사용하여 다양한 운영체제에서 소프트웨어를 테스트하기 위한 격리된 환경을 만든다.\n서버 통합: 조직에서는 KVM을 사용하여 여러 서버를 더 적은 수의 물리적 머신으로 통합하여 비용과 에너지를 절약한다.\n사전준비 말이 길었는데, 결국 k8s를 구성하는 Node에서 kvm을 지원 해줘야 OS(Linux)의 한계를 뛰어넘는 Container를 사용해 볼 수 있다.\nKVM 지원 확인 KVM 자체는 Linux Kernel에 이미 있기 때문에 하드웨어 가상화만 지원하면 된다. k8s Node에 터미널로 접속해 다음 명령어를 입력해보자.\n1 lsmod | grep kvm 다음과 같이 출력된다면 이미 활성화가 되어 있는 것이다.\n1 2 3 4 kvm_amd 208896 4 kvm 1409024 3 kvm_amd irqbypass 12288 1 kvm ccp 143360 4 kvm_amd Intel의 CPU를 사용한다면 kvm_intel이라고 출력될 것이다.\n출력이 안 되었다면, VMWare나 VirtualBox 설치할 때 처럼 해당 노드의 Bios에서 활성화를 해줘야 한다.\nWindows 컨테이너 정보 Windows 11을 설치할 것이다. 사양은 Windows 11 시스템 요구사항에 맞춰 구성했다.\n운영체제 : Windows 11 CPU Core : 2 메모리 : 4GB 저장공간 : 64GB kubernetes 구성 실제 구현은 CDK for Terraform Single Stack으로 작업했다.\nNamespace 생성 1 kubectl create namespace windows Windows 사용자 로그인 정보 Secret 생성 1 2 3 4 kubectl create secret generic windows-user-credential-secret \\ -n windows \\ --from-literal=username=\u0026lt;Windows 유저명\u0026gt; \\ --from-literal=password=\u0026lt;Windows 패스워드\u0026gt; 추가 Installation Script Configmap 생성 (선택사항) Container에 Windows가 처음 설치된 이후 1번 실행될 스크립트이다.\nContainer의 /oem/install.bat파일이 실행된다.\n1 2 3 . ├── install.bat └── remove-unnecessary-apps.ps1 이렇게 2개의 파일로 구성되어 있다. 각 파일의 내용은 다음과 같다.\ninstall.bat\n밑의 2번 PowerShell 파일을 실행하는 bat파일이다.\n1 2 3 4 5 @echo off echo Running initial installation scripts... powershell -ExecutionPolicy Bypass -File \u0026#34;%~dp0remove-unnecessary-apps.ps1\u0026#34; remove-unnecessary-apps.ps1\n불필요한 Windows 기본 앱들을 제거해주는 PowerShell 스크립트이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 C:\\WINDOWS\\System32\\OneDriveSetup.exe /uninstall $appNamesToDelete = @( \u0026#34;Clipchamp.Clipchamp\u0026#34;, \u0026#34;Microsoft.BingNews\u0026#34;, \u0026#34;Microsoft.BingSearch\u0026#34;, \u0026#34;Microsoft.BingWeather\u0026#34;, \u0026#34;Microsoft.GamingApp\u0026#34;, \u0026#34;Microsoft.MicrosoftOfficeHub\u0026#34;, \u0026#34;Microsoft.MicrosoftSolitaireCollection\u0026#34;, \u0026#34;Microsoft.MicrosoftStickyNotes\u0026#34;, \u0026#34;Microsoft.OutlookForWindows\u0026#34;, \u0026#34;Microsoft.PowerAutomateDesktop\u0026#34;, \u0026#34;Microsoft.Todos\u0026#34;, \u0026#34;Microsoft.WebMediaExtensions\u0026#34;, \u0026#34;Microsoft.Windows.Photos\u0026#34;, \u0026#34;Microsoft.WindowsAlarms\u0026#34;, \u0026#34;Microsoft.WindowsCalculator\u0026#34;, \u0026#34;Microsoft.WindowsCamera\u0026#34;, \u0026#34;Microsoft.WindowsFeedbackHub\u0026#34;, \u0026#34;Microsoft.WindowsSoundRecorder\u0026#34;, \u0026#34;Microsoft.Xbox.TCUI\u0026#34;, \u0026#34;MicrosoftCorporationII.QuickAssist\u0026#34;, \u0026#34;MSTeams\u0026#34;, \u0026#34;Microsoft.Copilot\u0026#34;, \u0026#34;Microsoft.ZuneMusic\u0026#34;, \u0026#34;Microsoft.ScreenSketch\u0026#34;, \u0026#34;Microsoft.WindowsAppRuntime.1.3\u0026#34;, \u0026#34;Microsoft.Paint\u0026#34;, \u0026#34;Microsoft.YourPhone\u0026#34;, \u0026#34;Microsoft.Windows.DevHome\u0026#34;, \u0026#34;Microsoft.XboxGamingOverlay\u0026#34;, \u0026#34;Microsoft.XboxSpeechToTextOverlay\u0026#34;, \u0026#34;Microsoft.WindowsStore\u0026#34;, \u0026#34;Microsoft.XboxIdentityProvider\u0026#34; ) Write-Host \u0026#39;Removing unnecessary apps...\u0026#39; foreach ($eachAppNameToDelete in $appNamesToDelete) { Get-AppxPackage | Where-Object { $_.Name -eq $eachAppNameToDelete } | Remove-AppxPackage } Write-Host \u0026#39;Unnecessary apps removal completed.\u0026#39; 이는 예시로, 원하는 대로 설치 스크립트를 구성할 수 있다.\n이 두 파일을 담는 ConfigMap을 생성한다.\n1 2 3 4 kubectl create configmap windows-oem-assets-configmap \\ -n windows \\ --from-file=install.bat \\ --from-file=remove-unnecessary-apps.ps1 PersistentVolumeClaim 메니페스트 파일 1 2 3 4 5 6 7 8 9 10 11 12 # pvc.yml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: windows-persistent-volume-claim namespace: windows spec: accessModes: - ReadWriteOnce resources: requests: storage: 64Gi Deployment 메니페스트 파일 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 # deployment.yml apiVersion: apps/v1 kind: Deployment metadata: name: windows-deployment namespace: windows spec: replicas: 1 selector: matchLabels: app: windows template: metadata: labels: app: windows spec: terminationGracePeriodSeconds: 120 containers: - name: windows image: dockurr/windows env: - name: VERSION value: \u0026#39;11\u0026#39; - name: CPU_CORES value: \u0026#39;2\u0026#39; - name: RAM_SIZE value: \u0026#39;4G\u0026#39; - name: DISK_SIZE value: \u0026#39;64G\u0026#39; - name: LANGUAGE value: \u0026#39;Korean\u0026#39; - name: REGION value: \u0026#39;ko-KR\u0026#39; - name: KEYBOARD value: \u0026#39;ko-KR\u0026#39; - name: USERNAME valueFrom: secretKeyRef: name: windows-user-credential-secret key: username - name: PASSWORD valueFrom: secretKeyRef: name: windows-user-credential-secret key: password ports: - name: http containerPort: 8006 protocol: TCP - name: rdp-tcp containerPort: 3389 protocol: TCP - name: rdp-udp containerPort: 3389 protocol: UDP - name: vnc containerPort: 5900 protocol: TCP securityContext: privileged: true capabilities: add: - NET_ADMIN volumeMounts: - name: windows-persistent-volume-claim mountPath: /storage - name: windows-oem-assets-configmap mountPath: /oem - name: dev-kvm mountPath: /dev/kvm - name: dev-tun mountPath: /dev/net/tun volumes: - name: windows-persistent-volume-claim persistentVolumeClaim: claimName: windows-persistent-volume-claim - name: windows-oem-assets-configmap configMap: name: windows-oem-assets-configmap - name: dev-kvm hostPath: path: /dev/kvm - name: dev-tun hostPath: path: /dev/net/tun type: CharDevice Service 메니페스트 파일 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # service.yml apiVersion: v1 kind: Service metadata: name: windows-service namespace: windows spec: selector: app: windows ports: - name: http port: 8006 targetPort: 8006 protocol: TCP - name: rdp-tcp port: 3389 targetPort: 3389 protocol: TCP - name: rdp-udp port: 3389 targetPort: 3389 protocol: UDP - name: vnc port: 5900 targetPort: 5900 protocol: TCP Ingress 메니페스트 파일 (선택) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # ingress.yml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: windows-ingress namespace: windows annotations: nginx.ingress.kubernetes.io/backend-protocol: \u0026#39;HTTP\u0026#39; nginx.ingress.kubernetes.io/rewrite-target: \u0026#39;/\u0026#39; # OAuth2 Proxy 사용 시 아래 주석 해제 # nginx.ingress.kubernetes.io/auth-url: \u0026#34;https://oauth2-proxy.example.com/oauth2/auth\u0026#34; # nginx.ingress.kubernetes.io/auth-signin: \u0026#34;https://oauth2-proxy.example.com/oauth2/start?rd=$scheme://$host$request_uri\u0026#34; spec: ingressClassName: nginx rules: - host: windows.yourdomain.com # 도메인으로 변경 http: paths: - path: / pathType: Prefix backend: service: name: windows-service port: number: 8006 매니페스트 배포 1 2 3 4 kubectl apply -f pvc.yml kubectl apply -f deployment.yml kubectl apply -f service.yml kubectl apply -f ingress.yml 배포 상태 확인 1 2 kubectl get pods -n windows kubectl logs -n windows -l app=windows -f 동작 테스트 웹 접속 ingress 설정까지 마쳤다면 연결한 도메인으로 접속해보자.\n별도 도메인이 없다면, 다음의 커맨드로 포트포워딩 후 localhost:8006으로 접속하자.\n1 2 3 4 kubectl port-forward \\ -n windows \\ --address localhost \\ svc/windows-service 8006:8006 처음에 Windows 설치파일을 다운로드 받는다 다운로드가 완료되면 설치가 시작된다 여기서 제법 시간이 오래 소요된다. 커피라도 한 잔 하고 오자.\n설치가 끝나면 Windows 바탕화면이 반겨준다 OEM 스크립트가 정상 동작했다면, 대부분의 불필요한 앱들이 삭제되었을 것이다. 원격 데스크탑(RDP) 접속 웹으로 보는 건 속도랑 반응성이 처참하므로 실 사용에서는 원격 데스크탑을 사용하는 것을 추천한다.\nService 구성에서 아예 NodePort로 3389 포트를 빼주거나 포트포워딩 해서 접속하는 것도 가능하다.\n포스트에 굳이 명시하진 않았는데, 내 경우 Nginx Ingress Controller가 LoadBalancer의 특정 포트를 할당하는 형태로 마무리 했다.\n원격 데스크톱 연결 리소스 소모량 Linux와는 근본적으로 상이한 OS가 컨테이너로 올라가다 보니 구체적으로 클러스터에 얼마나 부담이 되는지가 궁금해졌다.\n스토리지 LongHorn Volume 탭을 확인해보니니, 할당한 크기 64GB중 34GB 사용중으로 나온다.\n이 정도면 큰 문제는 없어 보인다 CPU / RAM Prometheus 기록상으론 CPU는 처음 설치 시점에만 높고 이후에는 잠잠해진다.\n근데 메모리는 할당한 4GB가 전부 로드되어 있다.\nWindows Container 내부에서도 메모리는 제법 많이 소모되고 있다.\nWindows 11 자체가 idle 상태에서도 생각보다 많은 메모리가 필요한 모양이다.\n좀 더 지켜봐야겠지만, 대처방안은 고려 해야겠다.\n필요할 때만 잠깐 배포하거나\n메모리 할당량을 늘리거나\n메모리를 잡아먹는 프로세스를 비활성화 하거나\n당장 생각할 수 있는 건 이 정도이다.\n마치며 보안에 대해 Windows라는 OS는 기본적으로 사용자명과 비밀번호 이 2가지로 인증 처리를 한다.\n따라서 어떤 형태든 추가적인 보안 레이어를 마련해 둘 필요가 있다.\n가장 좋은 방법은 애초에 외부 접근을 아예 못 하도록 설정하는 것이다.\nService를 보면 총 3개의 포트를 쓰는 것을 볼 수 있다.\nhttp rdp vnc http의 경우 내부적으로 noVNC로 서비스 되는데, 내 경우 ingress annotation에 oauth2 proxy를 설정해서 외부에서 아무나 들어올 수 없도록 추가적인 보안 레이어를 마련해두었다.\n문제는 RDP와 VNC이다. Public으로 이 2가지 프로토콜을 열어두는 건 아루미 생각해도 너무 꺼림칙해 아예 VPN을 통해서만 들어올 수 있도록 설정 해두었다.\n요컨대 Windows에 원격으로 접속/인증하는 방식이 근원적으로 안전하지 않기 때문에 반드시 이 점을 충분히 고려해서 서비스 네트워크를 구성하길 바란다.\n개인적인 의견 위 리소스 소모량 섹션에도 나와있듯이, idle 상태에서도 windows가 필요로 하는 메모리가 제법 많다는 걸 고려하면 비용 최적화 측면에서 그다지 현명한 접근 방법은 아니라고 본다.\n그렇다고 마냥 삐딱하게 볼 건 아니다.\nWindows나 MacOS에서만 동작하는 어떤 프로그램이 있다고 생각해보자.\n당신의 상사가 이를 k8s에 올려서 서비스 하라고 지시했다.\n만일 무슨 짓을 써도 매끄럽게 컨테이너화가 불가능한 상황이 온다면-\nKVM 기반의 컨테이너가 구원의 손길이 될지도 모른다. (물론 정식 라이센스 쓰고)\n","date":"2025-10-11T00:00:00+09:00","image":"https://blog.ayteneve93.com/p/dev/install-windows-on-k8s/images/cover_hu_65af170bfaf04ed8.png","permalink":"https://blog.ayteneve93.com/p/dev/install-windows-on-k8s/","title":"k8s에 Windows 설치하기"},{"content":"연관 포스트 Docling이란? 문제의 시작 회사에서 간단한 RAG 애플리케이션 을 하나 만들라는 지시를 받았다.\n여기엔 몇 가지 단서조항이 포함되어 있었는데, 문제가 되는 부분은 다음과 같다.\n운영 환경은 Windows 11 노트북\n네트워크에 연결되지 않은 상태 에서 동작해야 함\nDocker Desktop 설치하면 안 됨\n중국 기업에서 나온 모델은 사용하면 안 됨\n예를들어 DeepSeek QWEN 추석 연휴동안 작업 하려고 허락 받고 아예 집에 가져왔다. 노트북 사양 CPU/메모리는 괜찮다. 특히 메모리는 무려 64GB나 된다! 문제는 그래픽 카드인데, VRAM이 8GB 밖에 되지 않는다. 폐쇄망 이번 프로젝트에서 가장 큰 걸림돌이 바로 이것이다.\n네트워크가 안 되는 환경에서 구동 되어야 할 것\nOllama처럼 단순히 LLM을 설치하고 명령 받아서 처리만 해주는 컨테이너의 경우, 외부에서 제어만 잘 해주면 별다른 문제가 없겠으나 Docling처럼 AI가 애플리케이션 내부로 들어가서 겉에서 한 번 Wrapping된 형태라면 Offline 기능을 제공해주지 않는 이상 구현이 요원해진다.\n해결방안 컨테이너 구성 우선 전체적으로 컨테이너가 어떻게 구성되어 있는지 정리해두었다.\nHost OS가 Windows인 관계로, WSL 및 Docker와 Docker-Compose를 사용해서 컨테이너 환경을 마련했다.\n단서조항에 Docker Desktop은 설치하면 안 됨이 있어서 Ubuntu 위에 직접 설치했다.\nApplication Web: React 기반의 웹 애플리케이션 Backend: NestJs 기반의 API 서버 Infrastructure Ollama Image: ollama/ollama 용도: 텍스트 생성 / Embedding 사용된 모델 텍스트 생성: joonoh/HyperCLOVAX-SEED-Text-Instruct-1.5B:latest 임베딩: bona/bge-m3-korean:latest GPU 가속 : O Docling Image: quay.io/docling-project/docling-serve\n이 이미지는 CPU Only 모드로만 동작하는 Docling 컨테이너 이미지이다.\nGPU 가속이 가능한 이미지로도 써봤는데, VRAM 제한 때문에 CUDA Out of Memory 이슈와 함께 먹통이 되어버렸다.\n결국 이 프로젝트에서 GPU는 Ollama 컨테이너만 쓰는 것으로 타협을 봤다.\nVRAM에 여유가 있다면 다음의 Docker Image 중 하나를 골라 쓰면 된다.\nquay.io/docling-project/docling-serve-cu126: CUDA 12.6 quay.io/docling-project/docling-serve-cu128: CUDA 12.8 용도: Embedding 전처리\n사용된 모델\nds4sd/CodeFormulaV2: 수학 공식 분석 HuggingFaceTB/SmolVLM-256M-Instruct: 이미지 분석 sentence-transformers/all-MiniLM-L6-v2: 문서 Chunking GPU 가속 : X\nChroma Image: chromadb/chroma 용도: VectorStore Management Watchtower: 컨테이너 자동 업데이트 AutoHeal: HealthCheck Fail시 컨테이너 자동 리스타트 이미지에 AI Model을 내장하기 이번 문제의 핵심을 다시 한 번 요약하면 다음과 같다.\nRAG 애플리케이션을 Offline 상태의 노트북 1개에서 동작시켜야 한다.\n여기서 핵심이 되는 컨테이너는 Ollama와 Docling이다.\n두 컨테이너는 모두 AI 모델을 동적으로 다운받아 동작하는 것을 기본으로 한다.\n그럼, Ollama와 Docling에서 사용할 모델을 Docker Image에 내장하면 그만인 것 아닐까?\n방법 1) Docker Image에 모델 내장해서 올리기 우선 아예 모델 다운로드가 포함된 Image를 만들어서 Registry에 올려보았다.\n10GB가 넘는다. 심지어 필요한 모든 모델을 다 담은 것도 아니다. 당연한 얘기지만, 모델이 포함된 만큼 정직하게 크기가 늘어나버렸다.\n이게 비단 Docker 이미지가 좀 무거워졌다 수준의 문제가 아니다.\nCI/CD 파이프라인이 전반적으로 다 느려진다.\n이렇게 생성된 Image는 클라우드에서 제공하는 Container Registry에 올라가는데, 용량 때문에 비용 걱정도 해야 한다.\n방법 2) 빌드 타임에 AI 모델을 다운받도록 변경 생각해보니, 굳이 Image Registry에 올릴 필요는 없었다.\nOffline 환경에서 동작해야 한다이지,\nOffline 환경에서 설치해야 한다의 개념은 아니지 않은가.\nDocker Compose에 image 대신 build를 넣고 아예 Dockerfile 자체를 정의해주면 그만이다.\n그렇게 해서 나온 결과는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # docker-compose.yml services: # ...... # # Infrastructure Services ollama: build: context: ./build/ollama dockerfile: Dockerfile container_name: ollama restart: unless-stopped # Docker Compose에서 GPU를 할당할 땐 이런식으로 한다. deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] volumes: - Ollama.Data:/root/.ollama logging: options: max-size: 50m docling: container_name: docling build: context: ./build/docling dockerfile: Dockerfile restart: unless-stopped environment: DOCLING_SERVE_ENABLE_UI: \u0026#39;false\u0026#39; # (매우 중요!) 이 항목이 없으면 모델을 내장시켜도 Offline에서 자꾸 에러가 난다. HF_HUB_OFFLINE: 1 logging: options: max-size: 50m # ...... # volumes: Ollama.Data: build 디렉토리는 docker-compose.yml과 같은 경로에 배치 해두었다.\nbuild 디렉토리 내부 구조는 다음과 같다.\n1 2 3 4 5 6 build ├── docling │ └── Dockerfile └── ollama ├── Dockerfile └── entrypoint.sh 각 파일들은 다음과 같이 작성했다.\nbuild/docling/Dockerfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 FROM quay.io/docling-project/docling-serve-cpu # Docling의 기본 모델 다운로드 RUN docling-tools models download # Hybrid Chunker용 추가 모델 다운로드 RUN python3 -c \u0026#34;from transformers import AutoTokenizer, AutoModel; \\ AutoTokenizer.from_pretrained(\u0026#39;sentence-transformers/all-MiniLM-L6-v2\u0026#39;); \\ AutoModel.from_pretrained(\u0026#39;sentence-transformers/all-MiniLM-L6-v2\u0026#39;);\u0026#34; # 공식/이미지 분석용 모델 다운로드 RUN docling-tools models \\ download-hf-repo \\ ds4sd/CodeFormulaV2 \\ HuggingFaceTB/SmolVLM-256M-Instruct EXPOSE 5001 build/ollama/Dockerfile\n1 2 3 4 5 6 7 8 9 FROM ollama/ollama COPY ./entrypoint.sh /entrypoint.sh RUN chmod +x /entrypoint.sh ENTRYPOINT [\u0026#34;/entrypoint.sh\u0026#34;] EXPOSE 11434 Ollama는 Ollama 서버가 실행되고 나서야 모델을 받을 수 있으므로, 별도의 Entrypoint를 추가해줬다.\nbuild/ollama/entrypoint.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #!/bin/bash set -e echo \u0026#34;Starting Ollama server...\u0026#34; ollama serve \u0026amp; # 백그라운드에서 Ollama 실행 SERVER_PID=$! echo \u0026#34;Waiting for Ollama server to be active...\u0026#34; until ollama list \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; do # Ollama process가 정상적으로 동작 될 때까지 대기 sleep 1 done echo \u0026#34;Pulling models...\u0026#34; # For embedding ollama pull bona/bge-m3-korean:latest || true # For text generation ollama pull joonoh/HyperCLOVAX-SEED-Text-Instruct-1.5B:latest || true trap \u0026#34;kill -TERM $SERVER_PID\u0026#34; SIGTERM SIGINT wait $SERVER_PID 마치며 위 방법 2를 사용해서 Offline에서도 임베딩부터 텍스트 생성까지 정상 동작하는게 확인되었다.\n\u0026ldquo;Docling이란?\u0026rdquo; 포스트의 동작 테스트 문단에 실제로 테스트 한 영상을 올려두었다.\n","date":"2025-10-09T12:00:00+09:00","image":"https://blog.ayteneve93.com/p/dev/rag-app-in-an-offline-env/images/cover_hu_12d4923e660dd748.png","permalink":"https://blog.ayteneve93.com/p/dev/rag-app-in-an-offline-env/","title":"오프라인 환경에서 RAG 앱 동작시키기"},{"content":"연관 포스트 오프라인 환경에서 RAG 앱 동작시키기 RAG(Retrieval-Augmented Generation, 검색 증강 생성) 인공지능 시스템을 구축할 때, AI 모델(이하 LLM)에게 회사 내부 문서와 같이 사전에 학습되지 않은 정보를 활용한 답변을 기대한다고 생각해보자.\n1~2개 정도의 PDF 파일이라면 통째로 첨부해서 사용해도 상관 없겠지만,\n수십, 수백 개의 문서들을 모조리 LLM에게 입력할 수는 없는 노릇이다.\n이 문제를 해결하고자 나온 방법론 중 하나가 바로 RAG이다.\nRAG의 정의는 다음과 같다.\nLLM의 출력을 최적화하여 응답을 생성하기 전에 훈련 데이터 소스 외부의 신뢰할 수 있는 기술 자료를 참조하도록 하는 프로세스 소스가 되는 문서(가령 PDF 파일)를 AI가 쉽고 빠르게 관련성을 유추할 수 있는 형태의 데이터(vector)로 변환하고, 사용자의 질문(query)에 맞춰 검색된 데이터를 가져와(retrieving) prompt의 context로 넣어서 동작한다.\n이렇게 문자나 이미지같은 복잡한 데이터를 LLM이 이해하고 처리하기 쉬운 숫자 형태의 Vector로 변환하는 과정 혹은 변환된 결과물 그 자체를 Embedding이라고 하며, 그러한 작업을 수행하는 AI Model을 Embedding Model이라고 한다.\nEmbedding Model이 하는 일은 다음과 같다.\n텍스트/이미지 등을 Vector로 변환, VectorStore(혹은 Vector DB라고도 한다) 에 저장 사용자의 Query를 Vector로 변환, VectorStore에서 유사한 값들을 검색(Retrieval) Embedding 전처리 Chunking Embedding Model이 VectorStore에서 문서 데이터를 가져올 때, 가져온 결과 하나하나는 특별한 사유가 없는 한 있는 그대로 LLM에 전달된다.\n만일 텍스트로만 이루어진 어떤 문서의 글자 수가 5,000개이고, 이 5,000개가 한 덩어리로 VectorStore에 저장되어 있다면 LLM은 엄청난 크기의 Context에 적지 않은 부담을 지게 될 것이다. (혹은 당신의 지갑이\u0026hellip;)\n이에 따라 VectorStore에 문서를 저장할 땐 어떠한 방식으로든 원문을 잘게 잘라(chunking),\n사용자 Query와의 관련성은 유지하면서 불필요하게 많은 Context가 사용되는 일은 피하게 할 필요가 있다.\n1 2 3 4 5 6 7 8 9 10 11 [ { \u0026#34;pageContent\u0026#34;: \u0026#34;하지만 무작정 글자 수나 Token 수에 맞춰\u0026#34; }, { \u0026#34;pageContent\u0026#34;: \u0026#34;잘랐다가는, 이렇게 하나의 문장이 다 끝나\u0026#34; }, { \u0026#34;pageContent\u0026#34;: \u0026#34;기도 전에 잘려진 조각이 만들어질 것이다.\u0026#34; } ] 이와 같은 문제를 막겠다고 일부러 각 조각들 간 겹치는 부분을 만드는 게 일반적이나, 근본적인 해결책은 아니다.\n구조 분석 더욱이 원문이 PDF와 같이 구조화된 형태일 경우(이미지, 그래프, 테이블, 수식 등), 단순하게 텍스트만 추출하고\n구조는 무시해버린다면 최종적으로 LLM이 답변을 낼 때 전혀 엉뚱한 소리를 하는 경우가 생긴다.\n이런 식으로 Text로만 이루어진 경우는 오히려 드물다 Embedding 과정은 전통적으로\n원본 파일에서 Text만 추출 Token 수에 맞춰 쪼개기 Text를 Vectorizing VectorStore에 저장 이렇게 단순하게 이루어져 왔었다. 3/4번은 Embedding 모델이 하는 것이므로 여기선 논하지 않겠다.\n문제는 1/2번인데, 앞서 언급했듯 이렇게 텍스트만 추출하게 되면 원본 문서가 가지고 있던 구조적 특징이 유실되는 문제가 있다.\nPDF Parser나 OCR 같은 도구들을 활용해서 보완할 수는 있겠으나, PDF는 일반적으로 생각하는 것보다\n훨씬 복잡한 형태가 많고, 이를 완벽하게 추출해내는 것은 아직도 매우 어렵다.\nDocling(도클링)이란? Docling은 IBM Research에서 개발한 생성형 AI 애플리케이션을 위한 문서 처리 및 변환을 위한 오픈소스 툴킷이다.\nMIT 라이선스로 공개되어 있어 상업적으로도 자유롭게 활용할 수 있다.\n앞서 얘기한 Embedding 전처리 과정에서 생기는 문제점을 해결하고자 나온 오픈소스로,\n자체적인 인공지능 모델을 활용해 원본 문서를 분석/변환/Chunking 해준다.\n제공해주는 기능은 GitHub에 보다 잘 정리되어 있다.\n기본적으로 다양한 문서 포맷을 지원하고, Page Layout, Order, Table 등을 해석할 수 있으며, LangChain과 쉽게 통합 가능한 것이 특징이다.\n현재 이 글을 쓰고 있는 시점을 기준으로 언급된 기능들은 다음과 같다.\n🗂️ Parsing of multiple document formats incl. PDF, DOCX, PPTX, XLSX, HTML, WAV, MP3, VTT, images (PNG, TIFF, JPEG, \u0026hellip;), and more 📑 Advanced PDF understanding incl. page layout, reading order, table structure, code, formulas, image classification, and more 🧬 Unified, expressive DoclingDocument representation format ↪️ Various export formats and options, including Markdown, HTML, DocTags and lossless JSON 🔒 Local execution capabilities for sensitive data and air-gapped environments 🤖 Plug-and-play integrations incl. LangChain, LlamaIndex, Crew AI \u0026amp; Haystack for agentic AI 🔍 Extensive OCR support for scanned PDFs and images 👓 Support of several Visual Language Models (GraniteDocling) 🎙️ Audio support with Automatic Speech Recognition (ASR) models 🔌 Connect to any agent using the MCP server 💻 Simple and convenient CLI 다른 솔루션들과의 비교 문서 변환은 오랫동안 논의된 주제로, 이미 많은 솔루션이 존재한다. 최근 널리 사용되는 방식은 크게 두 가지로 나뉜다.\n1. VLM(Visual Language Model) 기반 솔루션 Closed-source: GPT-4, Claude, Gemini Open-source: LLaVA 기반 모델들 이러한 생성 모델 기반 솔루션은 강력하지만 다음과 같은 문제점이 있다:\n할루시네이션(Hallucination): 문서 변환 시 정확성이 중요한데, 모델이 존재하지 않는 내용을 생성할 수 있다 높은 계산 비용: 대규모 모델을 사용하기 때문에 비용이 매우 비싸고 비효율적이다 2. Task-specific 모델 기반 솔루션 대표 사례: Adobe Acrobat, Grobid, Marker, MinerU, Unstructured Docling의 접근 방식도 여기에 해당 이 방식은 OCR, 레이아웃 분석, 테이블 인식 등 특화된 모델들을 조합하여 사용한다.\n장점: 할루시네이션 문제가 적고, 정확하고 예측 가능한 변환 결과를 보장 단점: 상대적으로 커버리지가 작고, 다양한 특화 모델을 유지해야 하는 복잡성 Docling의 아키텍처 Docling은 크게 3가지 주요 컴포넌트로 구성되어 있다:\nPipelines: 문서 처리 파이프라인 Parser Backends: 다양한 문서 형식 처리기 DoclingDocument: Pydantic 기반의 통합 문서 표현 모델 Pipeline의 종류 1. StandardPdfPipeline\nPDF 및 이미지 입력을 DoclingDocument 형태로 변환하는 파이프라인 여러 AI 모델들을 단계적으로 사용하여 정보를 구조화 다음과 같은 특화 모델들을 활용: Layout Analysis Model: 페이지 내 각 요소들의 위치와 레이아웃 분석 TableFormer: 테이블 구조를 인식하고 복원 (행/열 정보 보존) OCR Engine: 스캔된 문서나 이미지 내 텍스트 추출 2. SimplePipeline\nPDF를 제외한 다른 문서 형식(DOCX, PPTX, HTML 등)을 처리 상대적으로 단순한 구조로, 빠른 처리가 가능 이러한 모듈 형태의 설계 덕분에 필요에 따라 각 단계를 교체하거나 확장할 수 있는 유연성을 제공한다.\n동작 테스트 오프라인 환경에서 RAG 앱 동작시키기 포스트에서 작업한 내용을 이쪽으로 가져왔다.\n테스트에 사용된 파일은 영화진흥위원회에서 공개한 25년 8월 영화산업 결산 보고서 PDF의 일부이다.\n텍스트 생성에 쓰인 LLM은 joonoh/HyperCLOVAX-SEED-Text-Instruct-1.5B:latest로\n예시로 쓰인 2025년 8월 대한민국 외국영화 흥행작 상위 10위에 대한 정보는 모델에 사전 학습되어 있는 것이 아니다.\n임베딩\n텍스트 생성 테스트\n마치며 프로젝트 제한사항으로 인해 Docling에서 GPU 가속을 못 쓰다 보니, 전반적으로 만족스러운 속도는 아니었다.\n하지만, 표 등이 포함된 소스 파일에서 단순한 텍스트 추출만 해서는 LLM이 이해할 수 있는 형태로\n전달되지 않았던 문제를 해결할 수 있는 좋은 방법이라고 생각한다.\n장단점 정리 장점:\nVLM 기반 솔루션 대비 훨씬 저렴한 비용 MIT 라이선스로 상업적 활용 자유 LangChain, LlamaIndex 등 주요 프레임워크와의 쉬운 통합 오프라인 환경(Air-gapped)에서도 사용 가능 Mac에서 MPS device를 활용한 빠른 처리 지원 단점:\nRuntime에 실시간으로 사용하기보단 RAG 인덱싱용으로 적합 VLM 대비 상대적으로 제한적인 커버리지 권장 사용 사례 Docling은 다음과 같은 경우에 특히 유용하다:\nRAG 시스템을 위한 문서 인덱싱 작업 테이블이나 복잡한 레이아웃이 포함된 PDF 처리 정확성이 중요한 문서 변환 작업 비용 효율적인 문서 처리 파이프라인 구축 이제까지 PDF Parser 같은 기본적인 라이브러리만 사용해봤다면, 한 번 결과를 보고 도입을 고려해봐도 괜찮을 것 같다.\n취향에 따라 Docker Container로 혹은 Python script에 모듈을 설치해 Import할 수도 있고, 아예 CLI로 동작시킬 수도 있다.\n","date":"2025-10-09T00:00:00+09:00","image":"https://blog.ayteneve93.com/p/dev/docling/images/cover_hu_68a25ccbddb1d0ba.png","permalink":"https://blog.ayteneve93.com/p/dev/docling/","title":"Docling이란?"},{"content":"개요 현재 이 블로그는 Hugo 프레임워크의 Stack Theme을 적용해서 빌드 되었다.\n이 글을 쓰고 있는 시간 기준으로 아직까지 포스트 된 글의 수가 많지는 않지만 추후 방문자 분들과 소통 해야 할 필요성이 있어 댓글 기능을 추가하고자 한다.\n댓글 시스템의 종류 Hugo Stack Theme의 가이드라인에 따르면 지원되는 댓글 시스템은 다음과 같다.\n시스템 데이터 저장소 주요 장점 주요 단점 Disqus Disqus 서버 기능 풍부, 사용자 익숙도 높음, 스팸 방지. 광고 (무료), 로딩 지연 가능성, 데이터 종속. DisqusJS Disqus 서버 Disqus 기능 + 성능 개선 (클릭 시 로드). 설정 복잡, 결국 Disqus에 의존. Cusdis Cusdis 서버/자체 호스팅 매우 경량, 프라이버시 우선 (광고/추적 없음). 기능 단순, UI 커스터마이징 제한. Twikoo 서버리스 DB 다양한 기능(이미지/Katex), 쉬운 서버리스 배포. 별도 서버리스 환경 구성 필요. Remark42 내 서버 (Boltdb) K8s/Docker에 최적화, 데이터 완전 소유, 프라이버시 보호. 초기 서버 설치 및 관리 필요. Cactus Matrix 네트워크 탈중앙화, 데이터 호스팅 주체 선택 가능. Matrix 계정 필요, 낮은 인지도. Giscus GitHub Discussions 서버 불필요, GitHub 리액션 활용, 경량. GitHub 계정 필수, 비기술 사용자 접근 어려움. Gitalk GitHub Issues 서버 불필요, 경량, 데이터 소유권. GitHub 계정 필수, 기능 단순. utterances GitHub Issues 가장 경량화, 깔끔한 GitHub 스타일 UI. GitHub 계정 필수, 기능 단순. Vssue 다중 Git 플랫폼 GitHub 외 GitLab/Bitbucket 지원, 서버 불필요. Git 계정 필수. Waline 서버리스 DB 라이트웨이트, 쉬운 배포, 다양한 형식 지원. 별도의 서버리스 백엔드 필요. 이 중 가장 경량화 된 방식인 utterances를 사용해서 댓글 시스템을 추가하고자 한다.\n추후 여유가 되면 Remark42 방식으로 k8s에 자체 호스팅을 할 계획이다.\n적용 Utterances 앱 설치 GitHub Marketplace 링크로 들어가 애플리케이션을 설치한다.\nOnly select repositories 선택 → 댓글을 저장할 Public Repository (보통 블로그 repo와 동일한 것으로 한다.)\n블로그 설정 파일 수정 Stack 테마의 경우 config/_default/params.toml 파일을 다음과 같이 수정해주면 된다.\n1 2 3 4 5 6 7 8 9 # config/_default/params.toml [comments] enabled = true provider = \u0026#34;utterances\u0026#34; [comments.utterances] repo = \u0026#34;ApexCaptain/ApexCaptain.github.io\u0026#34; # utterances 댓글이 저장될 저장소 issueTerm = \u0026#34;pathname\u0026#34; label = \u0026#34;comments\u0026#34; 로컬에서 블로그 가동 및 테스트 hugo 커맨드로 로컬 서버를 구동한다\n--cleanDestinationDir 플래그를 함께 넣어줘서 기존에 생성된 public, resources 폴더를 지우고 재생성 해주자.\n1 hugo server --cleanDestinationDir localhost:1313으로 접속해 아무 포스트나 들어가서 하단을 보면 다음과 같이 나온다.\n테스트로 댓글 하나를 달아보자.\n그러면 다음과 같이 markdown이 적용되어 h1 크기의 큼지막한 댓글이 달린 걸 확인 할 수 있다.\n위에서 지정한 대상 Repository의 Issue 탭을 확인 해보면 다음과 같이 Issue에 댓글이 추가되어 있다.\n","date":"2025-10-08T00:00:00+09:00","image":"https://blog.ayteneve93.com/p/blog/hugo/images/cover_hu_683e9d5b348b824a.png","permalink":"https://blog.ayteneve93.com/p/blog/hugo/","title":"Utterances로 블로그에 댓글 기능 추가하기"},{"content":"연관 포스트 Oracle Cloud Infrastructure 문제의 시작 OCI Instance(Node)의 Boot Volume 최소값 제한 OCI Free Tier에 따르면 OKE에서 가용할 수 있는 자원은 다음과 같다.\nCPU : 4개 메모리 : 24GB 블록 스토리지 : 200GB 이에 맞춰 Terraform으로 OKE 클러스터와 4개의 인스턴스를 가지는 ARM Node Pool을 생성했다.\n각 Node는 1개의 CPU와 6GB의 메모리를 가진다.\n스토리지의 경우 추후 PVC 생성을 위해서도 쓰이므로 최대한 작게 잡아주려고 했다.\nNode에는 최소한의 용량만, 나머지 대부분은 PVC로 할당 그런데 실제로 생성된 Instance의 정보를 보니 Boot Volume이 비정상적으로 크게 만들어져 있다.\nk8s Node의 역할을 하는 Oracle Instance 중 하나의 Boot Volume 정보 Boot Volume은 문자 그대로 Linux 시스템이 부팅하는데 필요한 기본 디스크로, 윈도우로 치면 C드라이브 같은 존재이다.\n확인 결과, 다음과 같은 이슈가 발생했다.\nInstance의 Boot Volume의 크기는 최소 47Gi (50GB) 이상 할당해줘야 한다.\n당연히 이 용량은 Free Tier에서 제공되는 200GB에서 차감된다.\n이러면 단순히 Node 4개를 만드는 것만으로도 Free Tier의 200GB를 다 써버리게 된다.\n단 1개의 PVC도 생성할 수 없다. Persistent Volume도 마찬가지 1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc spec: storageClassName: oci-bv #OCI에서 기본적으로 제공하는 Storage Class accessModes: - ReadWriteOnce resources: requests: storage: 100Mi 이처럼 OKE k8s 클러스터에 pvc 하나를 만든다고 가정해보자.\noci-bv는 Oracle에서 기본적으로 제공되는 Storage class로 OCI Block Volume을 저장소로 쓴다.\n약 100MB 정도의 크기를 가지는 작은 PVC이다. 그럼 실제 만들어지는 OCI Block Volume의 크기도 100MB 정도여야 하지 않을까?\n어림도 없다. 여기도 크기 제약이 걸려있어서 최소 47Gi (50GB) 이상을 할당해줘야 한다.\n극단적으로 Node를 1개만 쓴다고 가정해도, PVC는 3개가 한계이다. 각각 50GB씩\u0026hellip; 오라클, 보고 있나?\nPVC 자체도 50GB가 최소라 노드 1개 PVC 3개가 한계이다. 해결방안 NodePool의 스펙 및 수 조정 우선 Node에 50GB씩 기본 할당 되는 건 어쩔 도리가 없기 때문에 Node 수를 타협해야 한다.\n클러스터로써 구색은 맞춰야 하므로 기존 4개에서 2개로 줄였다.\n각각 CPU는 2개, 메모리는 12GB씩 할당해줬다.\n이걸로 벌써 100GB가 날아갔다 시스템 구성 요소 NFS Subdir External Provisioner를 사용해서 별도의 StorageClass를 만들어줘야 한다.\nNFS Storage, 즉 NFS 서버를 Source로 해서 StorageClass를 만들고, 해당 StorageClass를 통해 PVC를 생성하면 NFS 서버에 볼륨이 생성되고 데이터가 저장되는 구조이다.\n주요 구성 요소를 배포 순서에 따라 나열하면 다음과 같다.\nOCI Block Volume\n앞서 Node 2개를 배치했으므로, 사용 가능한 용량 제한은 100GB이다.\n남은 100GB를 모두 사용하는 OCI Block Volume 1개가 필요하다.\nNFS Server\n1에서 생성한 OCI Block Volume을 PVC로 매핑 받아 NFS Storage를 제공하는 서비스이다.\nNFS Subdir External Provisioner\n이번 포스트의 핵심으로, 2에서 생성한 NFS 서버를 Source로 StorageClass를 제공한다.\n(선택) FileBrowser, SFTP Container\n보다 편한 관리를 위한 것으로, 선택사항이다.\noci-bv가 제공하는 PVC는 ReadWriteOnce 모드만 제공되므로 반드시 하나의 Pod에 NFS Container와 함께 정의해줘야 한다.\n사용하는 도구 Terraform: 인프라 자원 관리 (OCI 볼륨, 백업 정책) Kubernetes Manifests: NFS 서버, File Browser, SFTP 서비스 Helm: NFS Subdir External Provisioner 설치 실제 구현은 CDKTF를 써서 하나의 Stack 파일로 구성했다.\nGitHub 링크에서 확인 가능하다.\nTerraform 인프라 구성 OCI 인프라 구성은 Terraform OCI Provider를 사용했다.\nHCL 코드에 프로바이더를 연결하는 과정은 여기선 생략한다. (추후 별도 포스팅 예정)\nOCI 블록 볼륨 1 2 3 4 5 6 7 8 9 10 11 # main.tf resource \u0026#34;oci_core_volume\u0026#34; \u0026#34;nfs_core_volume\u0026#34; { compartment_id = var.compartment_id availability_domain = var.availability_domain size_in_gbs = 100 display_name = \u0026#34;nfs-core-volume\u0026#34; lifecycle { prevent_destroy = true } } 볼륨 백업 정책 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # backup_policy.tf resource \u0026#34;oci_core_volume_backup_policy\u0026#34; \u0026#34;nfs_core_volume_backup_policy\u0026#34; { compartment_id = var.compartment_id display_name = \u0026#34;nfs-volume-backup-policy\u0026#34; schedules { backup_type = \u0026#34;INCREMENTAL\u0026#34; period = \u0026#34;ONE_WEEK\u0026#34; retention_seconds = 60 * 60 * 24 * 7 * 3 # 3주 보관 day_of_week = \u0026#34;SUNDAY\u0026#34; hour_of_day = 2 offset_seconds = 0 offset_type = \u0026#34;STRUCTURED\u0026#34; time_zone = \u0026#34;REGIONAL_DATA_CENTER_TIME\u0026#34; } schedules { backup_type = \u0026#34;FULL\u0026#34; period = \u0026#34;ONE_MONTH\u0026#34; retention_seconds = 60 * 60 * 24 * 30 * 2 # 2개월 보관 day_of_month = 1 hour_of_day = 3 offset_seconds = 0 offset_type = \u0026#34;STRUCTURED\u0026#34; time_zone = \u0026#34;REGIONAL_DATA_CENTER_TIME\u0026#34; } } 백업 스케줄:\n증분(INCREMENTAL) 백업: 매주 일요일 2시, 3주 보관 전체(FULL) 백업: 매월 1일 3시, 2개월 보관 백업 정책 할당 1 2 3 4 5 # backup_policy_assignment.tf resource \u0026#34;oci_core_volume_backup_policy_assignment\u0026#34; \u0026#34;nfs_core_volume_backup_policy_assignment\u0026#34; { asset_id = oci_core_volume.nfs_core_volume.id policy_id = oci_core_volume_backup_policy.nfs_core_volume_backup_policy.id } 변수 정의 1 2 3 4 5 6 7 8 9 10 # variables.tf variable \u0026#34;compartment_id\u0026#34; { description = \u0026#34;OCI Compartment ID\u0026#34; type = string } variable \u0026#34;availability_domain\u0026#34; { description = \u0026#34;OCI Availability Domain\u0026#34; type = string } 출력값 정의 1 2 3 4 5 # outputs.tf output \u0026#34;nfs_volume_id\u0026#34; { description = \u0026#34;NFS Core Volume OCID\u0026#34; value = oci_core_volume.nfs_core_volume.id } nfs_volume_id 값은 PV를 만들 때 필요하다.\n변수 파일 생성 1 2 3 4 cat \u0026gt; terraform.tfvars \u0026lt;\u0026lt; EOF compartment_id = \u0026#34;\u0026lt; 배포할 OCI Compartment의 ID \u0026gt;\u0026#34; availability_domain = \u0026#34;\u0026lt; AD 이름, 예시: ibHX:AP-CHUNCHEON-1-AD-1 \u0026gt;\u0026#34; EOF Terraform 배포 1 2 3 4 5 6 7 8 9 10 11 # Terraform 초기화 terraform init # 인프라 계획 확인 terraform plan -var-file=\u0026#34;terraform.tfvars\u0026#34; # 인프라 배포 terraform apply -var-file=\u0026#34;terraform.tfvars\u0026#34; # 출력값 확인 (볼륨 ID 등) terraform output k8s Manifest 네임스페이스 생성 1 2 3 4 5 # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: nfs-system PersistentVolume 생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # persistentvolume.yaml apiVersion: v1 kind: PersistentVolume metadata: name: nfs-pv annotations: pv.kubernetes.io/provisioned-by: blockvolume.csi.oraclecloud.com spec: storageClassName: oci-bv persistentVolumeReclaimPolicy: Retain capacity: storage: 100Gi accessModes: - ReadWriteOnce persistentVolumeSource: csi: driver: blockvolume.csi.oraclecloud.com volumeHandle: \u0026lt;OCI_VOLUME_OCID\u0026gt; # Terraform에서 Output 된 Volume ID값을 넣어준다. # ocid1.volume.oc1 어쩌구 하는 값이다. Web Console에서 복사해서 가져와도 된다. fsType: ext4 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: failure-domain.beta.kubernetes.io/zone operator: In values: - \u0026lt;AVAILABILITY_DOMAIN\u0026gt; PersistentVolumeClaim 생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # persistentvolumeclaim.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-pvc namespace: nfs-system spec: volumeName: nfs-pv # 위에서 생성한 PV의 이름이다 accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: oci-bv ConfigMap (Pod의 SFTP 컨테이너용 SSH 키 관리, 선택사항) 1 2 3 4 5 6 7 8 9 # configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: nfs-system-sftp-config namespace: nfs-system data: ssh-public-key: | ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQ... # SSH 공개키 Service 생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # service.yaml apiVersion: v1 kind: Service metadata: name: nfs-service namespace: nfs-system spec: selector: app: nfs ports: - name: nfs port: 2049 targetPort: 2049 protocol: TCP # -- 선택 사항 -- # - name: file-browser port: 8080 targetPort: 8080 protocol: TCP - name: sftp port: 22 targetPort: 22 protocol: TCP Deployment 생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nfs-deployment namespace: nfs-system spec: replicas: 1 selector: matchLabels: app: nfs template: metadata: labels: app: nfs spec: # FileBrowser를 안 쓸 거라면 initContainers는 필요 없음 initContainers: - name: init-filebrowser-db image: busybox:1.35 command: - /bin/sh - -c - | mkdir -p /exports/fb-database chown -R 1000:1000 /exports/fb-database chmod -R 755 /exports/fb-database volumeMounts: - name: nfs-storage mountPath: /exports containers: # NFS 서버 컨테이너 (핵심!!) - name: nfs-server image: itsthenetwork/nfs-server-alpine:latest-arm imagePullPolicy: Always ports: - containerPort: 2049 protocol: TCP securityContext: capabilities: add: - SYS_ADMIN - SETPCAP command: - /bin/sh - -c - | mkdir -p /exports/services /usr/bin/nfsd.sh volumeMounts: - name: nfs-storage mountPath: /exports env: - name: SHARED_DIRECTORY value: /exports - name: SHARED_DIRECTORY_2 value: /exports/services # File Browser 컨테이너(선택) - name: file-browser image: filebrowser/filebrowser imagePullPolicy: Always ports: - containerPort: 8080 protocol: TCP securityContext: runAsUser: 1000 runAsGroup: 1000 fsGroup: 1000 volumeMounts: - name: nfs-storage mountPath: /database subPath: fb-database - name: nfs-storage mountPath: /srv subPath: services env: - name: FB_NOAUTH value: \u0026#39;true\u0026#39; - name: FB_DATABASE value: /database/database.db - name: FB_PORT value: \u0026#39;8080\u0026#39; # SFTP 컨테이너(선택) - name: sftp-server image: jmcombs/sftp imagePullPolicy: Always command: - sh - -c - | chmod o+w /home/sftpuser/data /entrypoint sftpuser::::data ports: - containerPort: 22 protocol: TCP volumeMounts: - name: ssh-keys mountPath: /home/sftpuser/.ssh/keys readOnly: true - name: nfs-storage mountPath: /home/sftpuser/data subPath: services volumes: - name: nfs-storage persistentVolumeClaim: claimName: nfs-pvc - name: ssh-keys configMap: name: nfs-system-sftp-config 컨테이너별 역할\nNFS 서버: Alpine 기반 NFS 서버로 /exports 디렉토리 공유 File Browser: 웹 기반 파일 관리 인터페이스 제공 (선택) SFTP 서버: SSH 키 기반 파일 전송 서비스 (선택) FileBrowser Ingress 구성 (선택) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nfs-ingress namespace: nfs-system annotations: nginx.ingress.kubernetes.io/backend-protocol: HTTP nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: - host: files.example.com # 소유한 도메인으로 변경 http: paths: - path: / pathType: Prefix backend: service: name: nfs-service port: number: 8080 k8s 리소스 배포 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 네임스페이스 생성 kubectl apply -f namespace.yaml # ConfigMap 생성 kubectl apply -f configmap.yaml # PersistentVolume 생성 kubectl apply -f persistentvolume.yaml # PersistentVolumeClaim 생성 kubectl apply -f persistentvolumeclaim.yaml # Service 생성 kubectl apply -f service.yaml # Deployment 생성 kubectl apply -f deployment.yaml # Ingress 생성 (선택) kubectl apply -f ingress.yaml Helm Helm 저장소 추가 1 2 helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ helm repo update NFS Subdir External Provisioner Values 파일 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 nfs: # NFS 서버 서비스 주소 # 위 k8s manifest에서 생성한 nfs service의 k8s service dns를 사용했다. server: \u0026#39;nfs-service.nfs-system.svc.cluster.local\u0026#39; path: \u0026#39;/services\u0026#39; # 공유 경로 # StorageClass 설정 storageClass: # StorageClass 이름은 본인이 원하는대로 사용하면 된다 storageClassName: \u0026#39;nfs-client\u0026#39; accessModes: \u0026#39;ReadWriteMany\u0026#39; # PVC 할당 시 Storage에 저장 될 경로 패턴을 의미한다. # 예시의 경우 ./pvc/\u0026lt;네임스페이스 명\u0026gt;/\u0026lt;PVC 명\u0026gt;으로 저장된다. pathPattern: \u0026#39;.pvc/${.PVC.namespace}/${.PVC.name}\u0026#39; 보다 구체적인 Values 설정은 다음의 링크를 참조하길 바란다. Helm Chart 배포 1 2 3 4 5 6 7 8 helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --namespace nfs-system \\ --values values.yaml \\ --wait # 설치 상태 확인 helm list -n nfs-system helm status nfs-subdir-external-provisioner -n nfs-system 검증 및 테스트 리소스 상태 확인 1 2 3 4 5 6 7 8 9 10 11 # Pod 상태 확인 kubectl get pods -n nfs-system # Service 확인 kubectl get svc -n nfs-system # PVC 상태 확인 kubectl get pvc -n nfs-system # StorageClass 확인 kubectl get storageclass 마치며 이제 다른 네임스페이스에서 PVC를 생성해보자.\nFileBrowser와 ingress까지 설정했다면 웹상으로 접근해서 확인할 수 있다.\ningress가 별도로 없다면 다음 명령어로 포트포워딩해서 localhost:8080으로 접근해보자.\n1 kubectl port-forward --address localhost -n nfs-system svc/nfs-service 8080:8080 PVC가 지정된 경로 (.pvc/namespace/pvc-name)에 생성됨을 알 수 있다. 추후 계획 현재 k8s 클러스터에 Vault가 배포되어 있는데, Node의 수가 2개뿐이라 HA (High-Availability) 구성이 안 되고 있다. (최소 3개 이상 필요)\nNFS Provisioner는 외부에 존재하는 NFS Storage를 사용할 수도 있으므로, NAS용 컴퓨터 한 대를 구매해서 NFS 서버를 구축한 뒤 NFS Provisioner의 Source로 활용할 예정이다.\n이는 또 다른 클러스터인 On-Premise에도 적용될 예정이다. On-Premise 클러스터는 Longhorn을 설치해서 PVC를 제공하고 있는데, 이래저래 마음에 안 드는 구석이 많아 심플하게 외부 NAS로 통합하려고 한다.\nLonghorn에 대해서는 조만간 포스팅할 예정이다.\n","date":"2025-10-04T00:00:00+09:00","image":"https://blog.ayteneve93.com/p/dev/nfs-subdir-external-provisioner/images/cover_hu_3cdeb8e8c7ca709a.png","permalink":"https://blog.ayteneve93.com/p/dev/nfs-subdir-external-provisioner/","title":"Nfs subdir external provisioner"},{"content":"연관 포스트 Nfs subdir external provisioner\nOCI Block Volume 최소 크기 제한사항 해결\nKaaS (Kubernetes as a Service) 정의 Kubernetes를 클라우드에서 관리형 서비스로 제공하는 모델이다.\n특징 컨테이너화된 애플리케이션의 배포, 확장, 관리 등의 업무를 간소화 웹 콘솔, Terraform 등을 통해 k8s 클러스터를 자동화된 방식으로 구축 가능 클라우드와 통합된 StorageClass, LoadBalancer 제공 예시 가장 대표적인 Public Cloud Provider인 AWS, Azure, GCP에서는 다음과 같은 KaaS를 제공한다.\n항목 AWS EKS Azure AKS Google GKE 지원 버전 1.16.8 (2020년 5월) 1.18.1, 1.18.2 (2019년 5월) 1.16.8 (2020년 4월) 업데이트 Master 및 Node 자동 업데이트 Master 및 Node 온디맨드 업그레이드 Master CLI 업그레이드, Node 수동 업데이트 CLI 지원 지원 지원 지원 리소스 모니터링 Stackdriver Azure Monitor 타사 도구만 지원 Node 자동 확장 지원 프리뷰 단계 지원 Node 그룹 지원 미지원 지원 고가용성 지원 개발 중 지원 베어메탈 Node 미지원 미지원 AWS 제공 Master 업데이트 자동 수행 수동 수행 수동 수행 Node 업그레이드 자동 수행 수동 수행 관리형/비관리형 그룹 On-Premise AWS Outposts 지원 Anthos GKE Oracle Cloud Infrastructure Oracle Cloud Infrastructure(이하 OCI)는 Oracle에서 운영하는 Cloud Provider이다.\nAWS, Azure 등과 마찬가지로 OCI 역시 Oracle Kubernetes Engine(이하 OKE)라는 KaaS를 제공한다.\nOracle을 선택한 이유 OCI 자체는 다른 Cloud Provider들에 비해 특별한 장점이 있는 것은 아니다.\n하지만 개인적으로 Cloud에 k8s를 구성하길 희망한다면, 그에 부합하는 한 가지 커다란 이점이 있다.\nOracle에는 무려 상시 무료 서비스가 존재한다!\n클러스터 하나를 운영하기에는 부족함이 없다. AWS를 무료로 사용할 수 있는 기간은 1년이 한계이고, 그마저도 EKS는 포함되지도 않는다.\nEKS로는 Node 하나 없이 깡통 클러스터만 만들어 놔도 시간당 $0.1씩 과금된다.\n환율 1,400원 기준으로 계산하면 한 달에 무려 10만원씩 나간다!\n회사 차원에서 도입을 고려하는 경우라면, 이 정도 비용 격차는 그다지 큰 메리트는 아닐 것이다.\n오히려 후술한 단점들을 생각한다면 AWS나 Azure를 선택하는 것이 훨씬 합리적이다.\n하지만 개인 용도 + 학습을 목표로 하는 나와 같은 입장의 방문자가 있다면 썩 괜찮은 선택지이다.\n단점 레퍼런스 부족: 어디서 정보를 찾기가 너무 힘들다. 대부분의 개발자들은 그 존재조차 모르는 것 같다.\n부실한 공식 문서: 그럼 공식 문서라도 깔끔하게 되어있어야 하는데 그것도 아니다.\nAWS에 익숙해져서 그런 것도 있겠지만, OCI의 공식 문서는 객관적으로 봐도 가독성이 매우 떨어진다.\n공식 문서 사이트 링크가 있으니 궁금한 사람은 부디 들어가서 탐험 해보길 바란다.\nOCI Always Free 제한사항 OCI Free Tier 페이지에도 나와있는데, 모든 서비스가 무제한으로 사용 가능한 건 당연히 아니다.\n클러스터 1개를 운영한다고 가정하고, k8s에서 가용 가능한 자원을 간단하게 요약하면 다음과 같다.\n자원 유형 제한사항 Node Arm 기반 Ampere A1 코어 4개, 24GB 메모리 Persistent Volume 200GB Load Balancer Flexible Network Load Balancer 1개 Node의 경우 Node 1개당 최소 1개의 코어는 필요하므로 최대 사용 가능한 Arm Node 수는 4개이다.\nAMD Node도 있긴 한데, 사이즈가 너무 작아서 여기선 무시한다.\nOKE로 만드는 클러스터는 1개까지 무료로 사용 가능하다.\nOKE는 KaaS이기 때문에 별도로 Control-Plane Node가 필요 없다.\n따라서 4개의 Arm Node는 모두 Worker Node로 쓸 수 있다! 그것도 공짜로! 아마존, 보고 있나?\n목표 OCI Free Tier에 제한된 리소스 안에서 k8s 클러스터를 자체적으로 운영 추가 비용 X On-Premise 클러스터와 함께 멀티 클러스터 구성 ","date":"2025-10-03T00:00:00+09:00","image":"https://blog.ayteneve93.com/p/dev/oracle-cloud-infrastructure/images/cover_hu_c3944d0c7a0ed373.png","permalink":"https://blog.ayteneve93.com/p/dev/oracle-cloud-infrastructure/","title":"Oracle Cloud Infrastructure"},{"content":"문제의 시작 윈도우 11로 업데이트 하면서 몇 가지 불편해진 점이 있다.\n예를들어 :\n여러 압축 파일을 각각의 폴더에 해제 할 수 있던 기능이 없어졌다. 파일이나 폴더를 우클릭 -\u0026gt; 추가 옵션 표시를 눌러줘야 기존에 쓰던 기능을 쓸 수 있다. 그중 하나가 바로가기 파일이 윈도우 작업 표시줄에 드래그 앤 드롭으로 추가가 안 된다는 것이다.\n이번 포스트에선 Cursor나 VsCode 작업 영역(Workspace)의 바로가기를 만들고\n이를 작업 표시줄에 추가하는 방법을 공유하겠다.\n해결방안 Workspace 파일 생성 Cursor 혹은 VsCode로 Workspace파일을 생성한 후 원하는 이름으로 변경한다.\n생성한 Workspace 파일을 적당히 아무 위치로 옮겨준다.\n내 경우 C:\\에 넣어줬다.\n바로가기 생성 Workspace 파일의 바로가기를 만들어서 바탕화면으로 옮겨준다.\n하는 김에 아이콘도 이쁜 걸로 바꿔줬다.\n바로가기 파일 속성 편집 바로가기 파일 우클릭 -\u0026gt; 속성 -\u0026gt; 대상 값 앞에 explorer라고 추가해준다.\n작업 표시줄에 바로가기 등록 편집한 바로가기 파일을 작업 표시줄에 추가한다.\n마치며 집 메인 PC 바탕화면 작업 표시줄 오른쪽의 3개 아이콘은 모두 서로 독립된 개발 서버로 연결되는 Cursor Workspace이다.\n이중 하나를 누르면 SSH연결까지 자연스럽게 Cursor IDE로 접속된다.\n윈도우 검색으로도 들어갈 수 있다.\n추가한 파일 자체가 바로가기이기 때문에 비단 VsCode나 Cursor의 Workspace 뿐 아니라 모든 종류의 바로가기를 다 이런식으로 등록할 수 있다.\n","date":"2025-09-30T00:00:00+09:00","image":"https://blog.ayteneve93.com/p/etc/how-to-add-workspace-to-win11-taskbar/images/cover_hu_f458ffb83669f1ee.png","permalink":"https://blog.ayteneve93.com/p/etc/how-to-add-workspace-to-win11-taskbar/","title":"Win11 작업 표시줄에 Cursor Workspace 올리는 법"},{"content":"들어가기 앞서 회사와 무관하게 개인적으로 관리하는 k8s 클러스터는 다음과 같다.\n클라우드: OKE(EKS/GKE와 유사)처럼 기본 L4/L7 Load Balancer 제공 온프레미스: 집에서 운영하는 싱글 노드로 시작한 k8s, 리소스 집약적 워크로드 배치(Jellyfin, 7 Days To Die, Ollama, QBittorrent 등) Jellyfin같은 앱은 클라우드에 설치하기엔 자원 소모량이 너무 크다. 내부망 대역: 93.5.22.0/24 초기 Worker Node: 93.5.22.44 공유기 포트포워딩: 외부 80/443 → 93.5.22.44:80/443, 클러스터 내 Nginx Ingress Controller가 도메인 기반 라우팅 로드밸런서란?\n트래픽을 여러 노드/파드로 분산하고, 가상 IP(VIP)를 통해 단일 진입점을 제공 하드웨어 전용 장비도 있으나, 온프레미스 k8s에서는 소프트웨어 방식이 일반적 Bare Metal 환경에서는 Metallb가 사실상 표준 솔루션 Metallb 핵심 개념\n데몬셋 speaker가 호스트 네트워크로 동작하며 외부 IP를 네트워크에 광고 LoadBalancer 서비스의 External IP 전파에 표준 프로토콜 사용: ARP(IPv4), NDP(IPv6), BGP 두 가지 모드 L2(ARP/NDP): 같은 서브넷에서 VIP를 광고 — 가정/소규모 환경에 적합 BGP: 라우터와 경로를 교환 — 데이터센터/고급 네트워크에 적합 관련 개념 GARP(Gratuitous ARP): 자신의 IP-MAC 정보를 네트워크에 알리는 ARP, VIP 전환 시 필수 Strict ARP: 노드가 소유하지 않은 IP에 응답하지 않도록 제한, L2 모드와 IPVS에서 안전성 향상 문제의 시작 초기에는 단일 노드(93.5.22.44)로 포워딩하면 충분했지만, 노드가 추가되면 단일 대상 포워딩만으로는 고가용성/확장성 확보 불가 이런 상황이라면 공유기는 대체 어느 Node로 포워딩을 해야하는가? Kubernetes의 LoadBalancer 타입이 클라우드 밖에서는 기본 제공되지 않아, 외부에서 접근 가능한 VIP가 부재 결과적으로, 인입 트래픽을 안정적으로 받아 서비스로 라우팅할 수 있는 소프트웨어 로드밸런서가 필요 해결방안 — Metallb(L2 모드) 도입 같은 서브넷(93.5.22.0/24)에서 VIP를 광고하는 L2 모드로 간단하게 시작 예시 VIP: 93.5.22.100 (공유기 포트포워딩은 이 VIP로 설정) 설치(Helm) 1 2 3 4 helm repo add metallb https://metallb.github.io/metallb helm repo update kubectl create namespace metallb-system helm upgrade --install metallb metallb/metallb -n metallb-system --wait 설치 후 CRD 확인:\n1 kubectl get crd | grep metallb.io 다음과 같이 출력되면 정상이다. 1 2 3 4 5 6 7 8 bfdprofiles.metallb.io 2025-08-03T16:07:22Z bgpadvertisements.metallb.io 2025-08-03T16:07:22Z bgppeers.metallb.io 2025-08-03T16:07:22Z communities.metallb.io 2025-08-03T16:07:22Z ipaddresspools.metallb.io 2025-08-03T16:07:22Z l2advertisements.metallb.io 2025-08-03T16:07:22Z servicebgpstatuses.metallb.io 2025-08-03T16:07:22Z servicel2statuses.metallb.io 2025-08-03T16:07:22Z 이중 IPAddressPool과 L2Advertisement CRD를 Manifest로 생성 해줘야 한다. IPAddressPool: Metallb이 할당할 수 있는 IP 주소(VIP) 풀이다. L2Advertisement: Layer2 광고 정의. L2 모드에서 Metallb이 IP 주소를 어떻게 광고하는가를 설정한다. IP 풀/광고 리소스 생성 ipaddresspool.yaml:\n1 2 3 4 5 6 7 8 apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: workstation-ip-pool namespace: metallb-system spec: addresses: - 93.5.22.100 Note: addresses는 범위로 지정할 수도 있다. 예를들면 192.168.0.100-192.168.0.104\n적용:\n1 kubectl apply -f ipaddresspool.yaml l2advertisement.yaml:\n1 2 3 4 5 6 7 8 apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: workstation-l2adv namespace: metallb-system spec: ipAddressPools: - workstation-ip-pool 적용:\n1 kubectl apply -f l2advertisement.yaml 동작 확인 체크리스트 1 2 3 4 5 6 7 8 # 컨트롤러/스피커 파드 상태 kubectl get pods -n metallb-system -o wide # 풀/광고 리소스 상태 kubectl get ipaddresspools.metallb.io,l2advertisements.metallb.io -n metallb-system # LoadBalancer 서비스의 외부 IP 할당 여부 kubectl get svc -A | grep LoadBalancer || true 공유기 포트포워딩 외부 80/443 → VIP 93.5.22.100:80/443로 설정 참고(안전한 네트워킹을 위해) Strict ARP를 활성화하면, 노드가 소유하지 않은 IP에 응답하지 않아 ARP 오류를 방지 kube-proxy IPVS 모드 사용 시에도 Strict ARP는 중요 BGP 모드는 외부 라우터와의 동적 라우팅이 필요할 때 선택 추가로 Nginx Ingress Controller가 VIP로 들어온 요청을 각 서비스로 라우팅하도록 설정했다 마무리 온프레미스 k8s에서 Metallb는 외부 트래픽을 받는 가장 간단하고 표준적인 방법이다. 같은 서브넷에서는 L2 모드만으로도 VIP를 안전하게 광고하여, 노드 수가 늘어나도 일관된 진입점을 유지할 수 있다.\n","date":"2025-09-29T00:00:00+09:00","image":"https://blog.ayteneve93.com/p/dev/install-metallb-to-on-premise-k8s/images/cover_hu_e08fb3041537eb30.png","permalink":"https://blog.ayteneve93.com/p/dev/install-metallb-to-on-premise-k8s/","title":"On-Premise k8s에 Metallb 설치"},{"content":"\n⚠ 주의 : 본 포스트에서는 k8s에 VPN으로 우회되는 Torrent 서버 구성에 대한 정보를 담고 있습니다.\n부적절한 컨텐츠는 VPN이고 뭐고 절대 받지 맙시다! 👮\n문제의 시작 Jellyfin으로 지인들과 편하게 미디어를 함께 보고 싶어졌다. 귀칼이 그렇게 재밌다길래 미디어 콘텐츠를 구하는 가장 좋은 방법은 Torrent이다. 하지만 이를 위해 메인 PC를 하루종일 켜두고 싶지는 않다. 보안을 위해 VPN은 쓰되, 메인 PC 전체 트래픽을 VPN에 묶고 싶지는 않다. 해결방안 k8s 클러스터에 qBittorrent 배포\nVPN 구성을 위해 NordVPN의 서비스인 NordLynx를 적용\nIngress를 구성해서 보유한 도메인을 통해 접근 가능하도록 설정 (선택)\nqBittorrent의 기본 UI는 너무 못생겼으므로 UI도 VueTorrent로 변경 (선택)\n참고로 기본 UI는 이렇게 생겼다. VueTorrent UI는 이렇게 생겼다. 이쁘다! 구성 개요 시크릿: NordLynx 개인키(nord-lynx-private-key)를 담는 Opaque 타입 시크릿. 서비스(Service): ClusterIP로 qBittorrent 웹 및 토렌트 포트 노출. 디플로이먼트(Deployment): 사이드카 컨테이너 ghcr.io/bubuntux/nordlynx:latest (VPN) 메인 컨테이너 lscr.io/linuxserver/qbittorrent:latest initContainer로 커널 파라미터 설정(sysctl) NET_ADMIN capability 부여(터널 동작용) fsGroup: 1000으로 퍼미션 정리 Ingress(선택): NGINX Ingress 네임스페이스 생성 늘 그렇듯 처음은 Namespace부터 만든다. 이번 포스트에서는 일관성 있게 torrent라는 이름을 사용한다.\n1 kubectl create namespace torrent NordVPN Access Token 발급 NordVPN Dashboard에 접속 후 좌상단 NordVPN을 클릭한다. 2. 하단에 Get Access Token 버튼 클릭 3. 등록된 이메일로 인증코드 전송 4. 새 토큰을 생성한다 Private Key 생성 Docker 명령어를 통해 Private Key를 생성한다.\n\u0026lt;YOUR_ACCESS_TOKEN\u0026gt; 자리에 위에서 생성한 토큰 값을 넣어준다.\n1 docker run --rm --cap-add=NET_ADMIN -e TOKEN=\u0026lt;YOUR_ACCESS_TOKEN\u0026gt; ghcr.io/bubuntux/nordvpn:get_private_key | grep \u0026#34;Private Key:\u0026#34; | cut -d\u0026#39; \u0026#39; -f3 | tr -d \u0026#39;\\n\u0026#39; 시크릿 생성 \u0026lt;YOUR_PRIVATE_KEY\u0026gt; 자리를 위에서 Docker Command로 생성한 Private Key 값으로 교체한다.\n1 2 kubectl -n torrent create secret generic torrent-nord-lynx-private-key \\ --from-literal=nord-lynx-private-key=\u0026#39;\u0026lt;YOUR_PRIVATE_KEY\u0026gt;\u0026#39; Service 생성 service-qbittorrent.yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: Service metadata: name: qbittorrent namespace: torrent spec: type: ClusterIP selector: app: qbittorrent ports: - name: web port: 8080 targetPort: 8080 protocol: TCP - name: torrenting-tcp port: 6881 targetPort: 6881 protocol: TCP - name: torrenting-udp port: 6881 targetPort: 6881 protocol: UDP 적용:\n1 kubectl apply -f service-qbittorrent.yaml PVC 생성 StorageClass가 별도로 없다면 비워도 상관 없다.\n가급적 qbittorrent-complete는 HDD에 qbittorrent-incomplete는 SSD에 저장하는 것이 좋다.\npvc-qbittorrent.yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # qBittorrent의 설정을 저장하는 PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: qbittorrent-config namespace: torrent spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: \u0026lt;your-storage-class\u0026gt; # 클러스터 StorageClass 명에 맞게 수정 --- # 다운로드가 완료 된 파일들이 저장될 PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: qbittorrent-complete namespace: torrent spec: accessModes: - ReadWriteOnce resources: requests: storage: 500Gi # 용량은 본인이 원하는 만큼 할당 storageClassName: \u0026lt;your-storage-class\u0026gt; --- # 아직 다운로드 중인 파일들이 저장될 PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: qbittorrent-incomplete namespace: torrent spec: accessModes: - ReadWriteOnce resources: requests: storage: 200Gi # 용량은 본인이 원하는 만큼 할당 storageClassName: \u0026lt;your-storage-class\u0026gt; 적용:\n1 kubectl apply -f pvc-qbittorrent.yaml Deployment 생성 deployment-qbittorrent.yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 apiVersion: apps/v1 kind: Deployment metadata: name: qbittorrent namespace: torrent spec: replicas: 1 selector: matchLabels: app: qbittorrent template: metadata: labels: app: qbittorrent sidecar.istio.io/inject: \u0026#39;false\u0026#39; # Istio Sidecar와 NordLynx간의 충돌이 있다. spec: securityContext: fsGroup: 1000 initContainers: - name: init-sysctl image: busybox command: - /bin/sh - -c - | sysctl -w net.ipv6.conf.all.disable_ipv6=1 \u0026amp;\u0026amp; sysctl -w net.ipv4.conf.all.src_valid_mark=1 securityContext: privileged: true containers: - name: nordlynx image: ghcr.io/bubuntux/nordlynx:latest imagePullPolicy: Always env: - name: TZ value: Asia/Seoul - name: NET_LOCAL value: \u0026#39;10.244.0.0/16\u0026#39; # 클러스터 Pod CIDR에 맞게 수정 - name: ALLOW_LIST value: qbittorrent.torrent.svc.cluster.local - name: DNS value: \u0026#39;1.1.1.1,8.8.8.8\u0026#39; - name: PRIVATE_KEY valueFrom: secretKeyRef: name: torrent-nord-lynx-private-key key: nord-lynx-private-key - name: QUERY value: \u0026#39;filters[servers_groups][identifier]=legacy_p2p\u0026#39; - name: COUNTRY_CODE value: JP # 우회를 원하는 국가 코드 기입 securityContext: capabilities: add: - NET_ADMIN - name: web image: lscr.io/linuxserver/qbittorrent:latest imagePullPolicy: Always ports: - containerPort: 8080 protocol: TCP - containerPort: 6881 protocol: TCP - containerPort: 6881 protocol: UDP env: - name: PUID value: \u0026#39;1000\u0026#39; - name: PGID value: \u0026#39;1000\u0026#39; - name: TZ value: Asia/Seoul - name: WEBUI_PORT value: \u0026#39;8080\u0026#39; - name: TORRENTING_PORT value: \u0026#39;6881\u0026#39; # VueTorrent UI 적용 - name: DOCKER_MODS value: ghcr.io/gabe565/linuxserver-mod-vuetorrent volumeMounts: - name: qbittorrent-config mountPath: /config - name: qbittorrent-complete mountPath: /downloads - name: qbittorrent-incomplete mountPath: /downloads/incomplete volumes: # qBittorrent 설정 파일 - name: qbittorrent-config persistentVolumeClaim: claimName: qbittorrent-config # 다운로드가 완료된 파일들 - name: qbittorrent-complete persistentVolumeClaim: claimName: qbittorrent-complete # 다운로드 중인 파일들 - name: qbittorrent-incomplete persistentVolumeClaim: claimName: qbittorrent-incomplete 적용:\n1 kubectl apply -f deployment-qbittorrent.yaml Ingress 생성(선택) Ingress는 옵션이다. 실 사용에서는 Cloudflare 레코드로 도메인을 연결하고\nk8s 내부에는 nginx ingress controller를 설치해서 사용하고 있다.\n또한, 인증된 사용자만 접근할 수 있도록 별도로 OAuth2 Proxy로 보호받는다.\n여기에 대해서는 추후 포스팅 예정.\ningress-qbittorrent.yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: qbittorrent namespace: torrent annotations: nginx.ingress.kubernetes.io/backend-protocol: \u0026#39;HTTP\u0026#39; nginx.ingress.kubernetes.io/rewrite-target: \u0026#39;/\u0026#39; spec: ingressClassName: nginx rules: - host: torrent.your-domain.com # 실제로 연결할 도메인 선택 http: paths: - path: / pathType: Prefix backend: service: name: qbittorrent port: number: 8080 적용:\n1 kubectl apply -f ingress-qbittorrent.yaml 참고 NET_LOCAL은 클러스터 Pod CIDR에 맞춘다.\nPRIVATE_KEY는 위 시크릿 참조와 일치시킨다.\ninitContainer는 커널 파라미터를 조정하므로 privileged가 필요하다.\nNET_ADMIN capability가 없으면 VPN 터널이 정상 동작하지 않는다.\nVPN 네트워크가 끊어지면 모든 토렌트 서비스가 셧다운 된다.\nService Type은 Cluster IP로 설정했는데, 이는 Nginx Ingress Controller와 연결하기 위함이다.\n환경에 따라 NodePort나 LoadBalancer를 사용해서 접근 할 수 있도록 하자.\n마치며 WebUI에서 로그를 검색해보면 정상적으로 VPN IP가 할당되었음을 알 수 있다. 최종적으론 이렇게 나온다 ","date":"2025-09-29T00:00:00+09:00","image":"https://blog.ayteneve93.com/p/dev/torrenting-with-vpn-on-k8s/images/cover_hu_6ad701272a4bc66c.png","permalink":"https://blog.ayteneve93.com/p/dev/torrenting-with-vpn-on-k8s/","title":"VPN + qBittorrent 설치"},{"content":"GitHub 주요 목표 멀티 클러스터: OCI OKE(클라우드) + Workstation microk8s(온프레미스) GitOps 파이프라인: ArgoCD 중심의 선언적 배포 자동화 보안·신뢰: Bastion, OAuth2 Proxy, Cert-Manager, Vault 중심 시크릿·인증 체계 관찰성: Prometheus + Grafana 모니터링 스택 개인 미디어 인프라: Jellyfin, qBittorrent, 게임 서버 등 아키텍처 개요 네트워크(OCI): VCN + Public/Private/DB Subnet, LB/Bastion, K8s 노드, 데이터 계층 클러스터 계층 OKE(클라우드): System(NS)에 Istio, ArgoCD(개발 중), Vault(개발 중), 모니터링(개발 중), Ingress Workstation(microk8s): System(NS)에 Istio/모니터링/Longhorn, Application(NS)에 Dev/Media/Game/File 서비스 현재 성과(핵심) 인프라 자동화 OKE 및 Workstation 클러스터 구성 정의, 선택적 스택 배포/병렬화, 상태 백업 스크립트 제공 보안/신뢰 Bastion 접근 제어, OAuth2 Proxy, Cert-Manager 자동 인증서 발급 구성 Vault 기반 시크릿 관리 체계 설계(개발 중) 운영 효율 Prometheus/Grafana 관찰성 스택 구성(개발 중), 로그 중앙화 계획 개인 미디어/유틸 서비스(Jellyfin, qBittorrent, SFTP, 7 Days to Die) 구성 개발 생산성 src/terraform/stacks 중심 스택화 구조, scripts/ 내 배포 선택/백업/터미널 도구 Projen, ESLint/Prettier, Yarn 워크플로우 정착 진행 현황(요약) OKE 클러스터 자동 프로비저닝: 구성 Workstation microk8s 클러스터: 구성 Istio 서비스 메시: 구성 ArgoCD(GitOps): 개발 중 Vault(시크릿): 개발 중 Prometheus/Grafana(모니터링): 개발 중 Bastion/인증·인증서(Cert-Manager, OAuth2 Proxy): 구성 미디어/게임/SFTP 서비스: 구성 ","date":"2025-09-28T18:05:00+09:00","image":"https://blog.ayteneve93.com/p/dev/apexcaptain.iac/images/cover_hu_9fa4bc989c20081d.png","permalink":"https://blog.ayteneve93.com/p/dev/apexcaptain.iac/","title":"ApexCaptain.IaC"}]